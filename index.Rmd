---
title: "tfdatasets: R interface to TensorFlow Datasets API"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# tfdatasets: R interface to TensorFlow Datasets API

## Overview

The TensorFlow Datasets API provides various facilities for creating scalable input pipelines for TensorFlow models, including:

- Reading data from a variety of formats including CSV files and [TFRecords files](https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details) (the standard binary format for TensorFlow training data).

- Transforming datasets in a variety of ways including mapping arbitrary functions against them. 

- Shuffling, batching, and repeating datasets over a number of epochs.

- Streaming interface to data for reading arbitrarily large datasets.

- Reading and transforming data are TensorFlow graph operations, so are executed in C++ and in parallel with model training.

The R interface to TensorFlow datasets provides access to the Dataset API, including high-level convenience functions for easy integration with the [keras](https://tensorflow.rstudio.com/keras) and [tfestimators](https://tensorflow.rstudio.com/tfestimators) R packages.

## Installation

To use **tfdatasets** you need to install both the R package as well as [TensorFlow](https://rstudio.github.io/tensorflow/) itself.

First, install the tfdatasets R package as follows:

```{r, eval=FALSE}
devtools::install_github("rstudio/tfdatasets")
```

Then, use the `install_tensorflow()` function to install the current nightly build of TensorFlow:

```{r, eval=FALSE}
library(tfdtasets)
install_tensorflow(version = "nightly")
```

<div class="alert alert-warning" role="alert">
**IMPORTANT NOTE**: The **tfdatasets** package currently requires the nightly build of TensorFlow, it will not work with the release version as it relies on APIs only in the development version.
</div>

## Creating a Dataset

To create a dataset, use one of the [dataset creation](reference/index.html#section-creating-datasets) functions. Dataset can be created from delimted text files, [TFRecords files](https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details), as well as from in-memory data.

### Text Files

For example, to create a dataset from a text file, first create a specification for how records will be decoded from the file, then call `text_line_dataset()` with the file to be read and the specification:


```{r}
library(tfdatasets)

# create specification for parsing records from an example file
iris_spec <- csv_record_spec("iris.csv")

# read the datset
dataset <- text_line_dataset("iris.csv", record_spec = iris_spec)
```

In the example above, the [csv_record_spec()] function is passed an example file which is used to  automatically detect column names and types (done by reading up to the first 1,000 lines of the file). You can also provide explicit column names and/or data types using the `names` and `types` parameters (note that in this case we don't pass an example file):

```{r}
# provide colum names and types explicitly
iris_spec <- csv_record_spec(
  names = c("SepalLength", "SepalWidth", "PetalLength", "PetalWidth", "Species"),
  types = c("double", "double", "double", "double", "integer"), 
  skip = 1
)

# read the datset
dataset <- text_line_dataset("iris.csv", record_spec = iris_spec)
```

Note that we've also specified `skip = 1` to indicate that the first row of the CSV that contains column names should be skipped.

Supported column types are integer, double, and character. You can also provide `types` in a more compact form using single-letter abbreviations (e.g. `types = "dddi"`). For example:

```{r}
mtcars_spec <- csv_record_spec("mtcars.csv", types = "dididddiiii")
```

#### Parallel Decoding

Decoding lines of text into a record can be computationally expensive. You can parallelize these computations using the `parallel_records` parameter. For example:

```{r}
dataset <- text_line_dataset("iris.csv", record_spec = iris_spec, parallel_records = 4)
```

You can also parallelize the reading of data from storage by requesting that a buffer of records be prefected. You do this with the [dataset_prefetch()] function. For example:

```{r}
dataset <- text_line_dataset("iris.csv", record_spec = iris_spec, parallel_records = 4) %>% 
  dataset_prefetch(1000)
```

If you have multiple input files, you can also parallelize reading of these files both across multiple machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). See the section on [Reading Multiple Files] below for additional details.



## Transformations

### Mapping

You can map arbitrary transformation functions onto dataset records using the `dataset_map()` function. For example, to transform the "Species" column into a one-hot encoded vector you would do this:

```{r}
dataset <- dataset %>% 
  dataset_map(function(record) {
    record$Species <- tf$one_hot(record$Species, 3L)
    record
  })
```

Note that while `dataset_map()` is defined using an R function, there are some special constraints on this function which allow it to execute *not within R* but rather within the TensorFlow graph. 

For a dataset created with the `csv_dataset()` function, the passed record will be named list of tensors (one for each column of the dataset). The return value should be another set of tensors which were created from TensorFlow functions (e.g. `tf$one_hot` as illustrated above). This function will be converted to a TensorFlow graph operation that performs the transformation within native code. 

#### Parallel Mapping

If these transformations are computationally expensive they can be executed on multiple threads using the `num_parallel_calls` parameter. For example:

```{r}
dataset <- dataset %>% 
  dataset_map(num_parallel_calls = 4, function(record) {
    record$Species <- tf$one_hot(record$Species, 3L)
    record
  })
```

You can control the maximum number of processed elements that will be buffered when processing in parallel using the `dataset_prefetch()` transformation. For example:

```{r}
dataset <- dataset %>% 
  dataset_map(num_parallel_calls = 4, function(record) {
    record$Species <- tf$one_hot(record$Species, 3L)
    record
  }) %>% 
  datset_prefetch(100)
```


### Filtering

You can filter the elements of a dataset using the `dataset_filter()` function, which takes a `predicate` function that returns a boolean tensor for records that should be included. For example:

```{r}
dataset <- csv_dataset("mtcars.csv") %>%
  dataset_filter(function(record) {
    record$mpg >= 20
})

dataset <- csv_dataset("mtcars.csv") %>%
  dataset_filter(function(record) {
    record$mpg >= 20 & record$cyl >= 6L
  })
```

Note that the functions used inside the predicate must be tensor operations (e.g. `tf$not_equal`, `tf$less`, etc.). R generic methods for relational operators (e.g. <, >, <=, etc.) and logical operators (e.g. !, &, |, etc.) are provided so you can use shorthand syntax for most common comparisions (as illustrated above).


### Features and Response 

A common transformation is taking a column oriented dataset (e.g. one created by `csv_dataset()` or `tfrecord_dataset()`) and transforming it into a two-element list with features ("x") and response ("y"). You can use the `dataset_prepare()` function to do this type of transformation. For example:

```{r}
mtcars_dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl)

iris_dataset <- text_line_dataset("iris.csv", record_spec = iris_spec) %>% 
  dataset_prepare(x = -Species, y = Species)
```

The `dataset_prepare()` function also accepts standard R formula syntax for defining features and response:

```{r}
mtcars_dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_prepare(cyl ~ mpg + disp)
```


### Shuffling and Batching 

There are several functions which control how batches are drawn from the dataset. For example, the following specifies that data will be drawn in batches of 128 from a shuffled window of 1000 records, and that the dataset will be repeated for 10 epochs:

```{r}
dataset <- dataset %>% 
  dataset_shuffle(1000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(10)
```


### Complete Example

Here's a complete example of using the various dataset transformation functions together. We'll read the `mtcars` dataset from a CSV, filter it on some threshold values, map it into `x` and `y` components for modeling, and specify desired shuffling and batch iteration behavior:

```{r}
dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>%
  dataset_prefetch(1000) %>% 
  dataset_filter(function(record) {
    record$mpg >= 20 & record$cyl >= 6L
  }) %>% 
  dataset_prepare(cyl ~ mpg + disp) %>% 
  dataset_shuffle(1000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(10)
```

## Reading Datasets

You read batches of data from a dataset by using tensors that yield the next batch. You can obtain this tensor from a dataset via the `next_batch()` function. For example:

```{r}
dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_prepare(cyl ~ mpg + disp) %>% 
  dataset_shuffle(20) %>% 
  dataset_batch(5)
batch <- next_batch(dataset)
batch
```
```
$x
Tensor("IteratorGetNext_13:0", shape=(?, 2), dtype=float32)

$y
Tensor("IteratorGetNext_13:1", shape=(?,), dtype=int32)
```

As you can see `batch` isn't the data itself but rather a tensor that will yield the next batch of data when it is evaluated:

```{r}
sess <- tf$Session()
sess$run(batch)
```
```
$x
     [,1] [,2]
[1,] 21.0  160
[2,] 21.0  160
[3,] 22.8  108
[4,] 21.4  258
[5,] 18.7  360

$y
[1] 6 6 4 6 8
```

## Dataset Iteration

If you are iterating over an entire dataset by evaluating the `next_batch()` tensor you will need to determine at what point to stop iteration. There are a couple of possible approaches to controlling/detecting when iteration should end.

One approach is to create a dataset that yields batches infinitely (traversing the dataset multiple times with different batches randomly drawn). In this case you'd use another mechanism like a global step counter or detecting a learning plateau:

```{r}
library(tfdatasets)
dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl) %>% 
  dataset_shuffle(5000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat() # repeat infinitely

batch <- next_batch(dataset)

steps <- 200
for (i in 1:steps) {
  # use batch$x and batch$y tensors
}
```

The call to `dataset_repeat()` with no `count` parameter requests that the dataset be traversed infinitely.

Another approach is to detect when all batches have been yielded from the dataset. When the tensor reaches the end of iteration a runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in the `with_dataset()` function:

```{r}
library(tfdatasets)
dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(10)
  
batch <- next_batch(dataset)

with_dataset({
  while(TRUE) {
    # use batch$x and batch$y tensors
  }
})
```


## Using with tfestimators

Models created with **tfestimators** use an input function to consume data for training, evaluation, and prediction. For example, here is an example of using an input function to feed data from an in-memory R data frame to an estimators model:

```{r}
model %>% train(
  input_fn(mtcars, features = c(mpg, disp), response = cyl,
           batch_size = 128, epochs = 3)
)
```

If you are using **tfdatasets** with the **tfestimators** package, you can create an estimators input function directly from a dataset as follows:

```{r}
dataset <- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(3)

model %>% train(
  input_fn(dataset, features = c(mpg, disp), response = cyl)
)
```

Note that we don't use the `dataset_prepare()` or `next_batch()` functions in this example. Rather, these functions are used under the hood to provide the `input_fn` interface expected by tfestimators models.

As with `dataset_prepare()`, you can alternatively use an R formula to specify features and response:

```{r}
model %>% train(
  input_fn(dataset, cyl ~ mpg + disp)
)
```

## Using with Keras

Keras models are often trained by passing in-memory arrays directly to the `fit` function. For example:

```{r}
model %>% fit(
  x_train, y_train, 
  epochs = 30, 
  batch_size = 128
)
```

However, this requires loading data into an R data frame or matrix before calling fit. You can use the `train_on_batch()` function to stream data one batch at a time, however the reading and processing of the input data is still being done serially and outside of native code.

Alternatively, Keras enables you to wire input and output tensors directly into the model definition, which are then evaluated for each training step. You can combine this capability with `dataset_prepare()` and `next_batch()` to efficiently stream data into Keras training operations. Here is a complete example:

```{r}
library(keras)
library(tfdatasets)
  
# create dataset that yields batches infinitely
dataset <- text_line_dataset("iris.csv", record_spec = iris_spec) %>%
  dataset_map(function(record) {
    record$Species <- tf$one_hot(record$Species, depth = 3L)
    record
  }) %>%
  dataset_prepare(x = -Species, y = Species) %>% 
  dataset_shuffle(1000) %>%
  dataset_batch(128) %>%
  dataset_repeat() 

# stream batches from dataset
batch <- next_batch(dataset)

# create model
input <- layer_input(tensor = batch$x, shape = c(4))
predictions <- input %>%
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 20, activation = "relu") %>%
  layer_dense(units = 3, activation = "softmax")
model <- keras_model(input, predictions)
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy'),
  target_tensors = batch$y
)

# fit the model
model %>% fit(
  steps_per_epoch = 30,
  epochs = 5
)
```

Note that we don't pass `x_train` or `y_train` to `fit()`, rather the feature data (`batch$x`) is provided as the `tensor` argument to `layer_input()` and the response data (`batch$y`) is provided as the `target_tensors` argument to `compile()`. The training data is directly wired into the TensorFlow graph built for the model.

Note also that rather than calling the Keras `to_categorical()` function to one-hot encode the "Species" field, we do this instead in a `dataset_map()` operation that calls the `tf$one_hot()` function. 

## Reading Multiple Files

If you have multiple input files you can process them in parallel both across machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). The [read_files()] function provides a high-level interface to parallel file reading. 

The [read_files()] function takes a set of files and a read function along with various options to orchestrate parallel reading. For example, the following function reads all CSV files in a directory using the [text_line_dataset()] function:

```{r}
dataset <- read_files("data/*.csv", text_line_dataset, record_spec = mtcars_spec,
                      parallel_files = 4, parallel_interleave = 16) %>% 
  dataset_shuffle(1000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(3)
```

The `parallel_files` argument requests that 4 files be processed in parallel and the `parallel_interleave` argument requests that blocks of 16 consecutive records from each file be interleaved in the resulting dataset.

If you are training on multiple machines and the training supervisor passes a shard index to your training script, you can also parallelizing reading by sharding the file list. For example:

```{r}
# command line flags for training script (shard info is passed by training 
# supervisor that executes the script)
FLGAS <- flags(
  flag_integer("num_shards", 1),
  flag_integeR("shard_index", 1)
)

# forward shard info to read_files
dataset <- read_files("data/*.csv", text_line_dataset, record_spec = mtcars_spec,
                      num_shards = FLAGS$num_shards, shard_index = FLAGS$shard_index,
                      parallel_files = 4, parallel_interleave = 16) %>% 
  dataset_shuffle(1000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(3)
```







