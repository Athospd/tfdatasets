---
title: "tfdatasets: R interface to TensorFlow Datasets API"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# tfdatasets: R interface to TensorFlow Datasets API

## Overview

The TensorFlow Datasets API provides various facilities for creating scalable input pipelines for TensorFlow models, including:

- Reading data from a variety of formats including CSV files and [TFRecords files](https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details) (the standard binary format for TensorFlow training data).

- Transforming datasets in a variety of ways including mapping arbitrary functions against them. 

- Shuffling, batching, and repeating datasets over a number of epochs.

- Streaming interface to data for reading arbitrarily large datasets.

- Reading and transforming data are TensorFlow graph operations, so are executed in C++ and in parallel with model training.

The R interface to TensorFlow datasets provides access to the Dataset API, including high-level convenience functions for easy integration with the [keras](https://tensorflow.rstudio.com/keras) and [tfestimators](https://tensorflow.rstudio.com/tfestimators) R packages.

## Installation

To use **tfdatasets** you need to install both the R package as well as [TensorFlow](https://rstudio.github.io/tensorflow/) itself.

First, install the tfdatasets R package as follows:

```{r, eval=FALSE}
devtools::install_github("rstudio/tfdatasets")
```

Then, use the `install_tensorflow()` function to install the current nightly build of TensorFlow:

```{r, eval=FALSE}
library(tfdtasets)
install_tensorflow(version = "nightly")
```

<div class="alert alert-warning" role="alert">
**IMPORTANT NOTE**: The **tfdatasets** package currently requires the nightly build of TensorFlow, it will not work with the release version as it relies on APIs only in the development version.
</div>

## Creating a Dataset

To create a dataset, use one of the [dataset creation](reference/index.html#section-creating-datasets) functions. For example, to create a dataset from a CSV file:


```{r}
library(tfdatasets)
dataset <- csv_dataset("iris.csv")
```

Dataset columns and data types are detected automatically by reading up to the first 1,000 lines of the CSV file. You can provide explicit column names and/or data types using the `col_names` and `col_types` parameters:

```{r}
dataset <- csv_dataset("iris.csv",
  col_names = c("SepalLength", "SepalWidth", "PetalLength", "PetalWidth", "Species"),
  col_types = c("double", "double", "double", "double", "integer"), skip = 1
)
```

Supported column types are integer, double, and character. You can also provide `col_types` in a more compact form using single-letter abbreviations (e.g. `col_types = "dddi"`).

Note that we've also specified `skip = 1` to indicate that the first row of the CSV that contains column names should be skipped.

## Transformations

You can map arbitrary transformation functions onto dataset records using the `dataset_map()` function. For example, to transform the "Species" column into a one-hot encoded vector you would do this:

```{r}
dataset <- dataset %>% 
  dataset_map(function(record) {
    record$Species <- tf$one_hot(record$Species, 3L)
    record
  })
```

Note that while `dataset_map()` is defined using an R function, there are some special constraints on this function which allow it to execute *not within R* but rather within the TensorFlow graph. 

For a dataset created with the `csv_dataset()` function, the passed record will be named list of tensors (one for each column of the dataset). The return value should be another set of tensors which were created from TensorFlow functions (e.g. `tf$one_hot` as illustrated above). This function will be converted to a TensorFlow graph operation that performs the transformation within native code. 

### Parallel Mapping

If these transformations are computationally expensive they can be executed on multiple threads using the `num_parallel_calls` parameter. For example:

```{r}
dataset <- dataset %>% 
  dataset_map(num_parallel_calls = 4, function(record) {
    record$Species <- tf$one_hot(record$Species, 3L)
    record
  })
```

Note that the `csv_dataset()` function described above is just a wrapper around a `text_line_dataset()` and the `dataset_decode_delim()` function, which maps text strings into named columns. It can therefore also perform CSV parsing in parallel with `num_parallel_calls`:

```{r}
dataset <- csv_dataset("iris.csv", num_parallel_calls = 4)
```

### Features and Response 

A common tranformation is taking a column oriented dataset (e.g. one created by `csv_dataset()` or `tfrecord_dataset()`) and transforming it into a two-element list with features ("x") and response ("y"). You can use the `dataset_prepare()` function to do this type of transformation. For example:

```{r}
mtcars_dataset <- csv_dataset("mtcars.csv") %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl)

iris_dataset <- csv_dataset("iris.csv") %>% 
  dataset_prepare(x = -Species, y = Species)
```

## Shuffling and Batching 

There are several functions which control how batches are drawn from the dataset. For example, the following specifies that data will be drawn in batches of 128 from a shuffled buffer of 5000 records, and that the dataset will be repeated for 10 epochs:

```{r}
dataset <- dataset %>% 
  dataset_shuffle(5000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(10)
```

## Reading Datasets

To read batches from a dataset you obtain an iterator for the dataset and then evaluate a "get next" tensor which yields the next batch. You can use the `iterator_get_next()` function to obtain this tensor from a dataset:

```{r}
dataset <- csv_dataset("mtcars.csv") %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl) %>% 
  dataset_batch(5)
batch <- iterator_get_next(dataset)
batch
```
```
$x
Tensor("IteratorGetNext_13:0", shape=(?, 2), dtype=float32)

$y
Tensor("IteratorGetNext_13:1", shape=(?,), dtype=int32)
```

As you can see "batch" isn't the data itself but rather a tensor that will yield the next batch of data when it is evaluated:

```{r}
sess <- tf$Session()
sess$run(batch)
```
```
$x
     [,1] [,2]
[1,] 21.0  160
[2,] 21.0  160
[3,] 22.8  108
[4,] 21.4  258
[5,] 18.7  360

$y
[1] 6 6 4 6 8
```

See the [Batch Iteration] section below for various ways to iterate over batches within a dataset.

## Using with tfestimators

Models created with **tfestimators** use an input function to consume data for training, evaluation, and prediction. For example, here is an example of using an input function to feed data from an R data frame to a tfestimators model:

```{r}
model %>% train(
  input_fn(mtcars, features = c(drat, cyl), response = mpg,
           batch_size = 128, epochs = 3)
)
```

To use **tfdatasets** with a dataset streamed from a CSV instead you would do the following:

```{r}
dataset <- csv_dataset("mtcars.csv") %>% 
  dataset_batch(128) %>% 
  dataset_repeat(3)

model %>% train(
  input_fn_from_dataset(dataset, features = c(drat, cyl), response = mpg)
)
```

Note that we don't use `dataset_prepare()` and `iterator_get_next()` in this example. Rather, these functions are used by `input_fn_from_dataset()` under the hood to provide the `input_fn` interface expected by tfestimators models.

## Using with Keras

Keras models are often trained by passing in-memory arrays directly to the `fit` function. For example:

```{r}
model %>% fit(
  x_train, y_train, 
  epochs = 30, 
  batch_size = 128
)
```

However, this requires loading data into an R data frame or matrix before calling fit. You can use the `train_on_batch()` function to stream data one batch at a time, however the reading and processing of the input data is still being done serially and outside of native code.

Alternatively, Keras enables you to wire input and output tensors directly into the model definition, which are then evaluated for each training step. You can combine this capability with `dataset_prepare()` and `iterator_get_next()` to efficiently stream data into Keras training operations. Here is a complete example:

```{r}
library(keras)
library(tfdatasets)
  
# create dataset that yields batches infinitely
dataset <- csv_dataset("iris.csv") %>%
  dataset_map(function(record) {
    record$Species <- tf$one_hot(record$Species, depth = 3L)
    record
  }) %>%
  dataset_prepare(x = -Species, y = Species) %>% 
  dataset_shuffle(1000) %>%
  dataset_batch(128) %>%
  dataset_repeat() 

# stream batches from dataset
batch <- iterator_get_next(dataset)

# create model
input <- layer_input(tensor = batch$x, shape = c(4))
predictions <- input %>%
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 20, activation = "relu") %>%
  layer_dense(units = 3, activation = "softmax")
model <- keras_model(input, predictions)
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy'),
  target_tensors = batch$y
)

# fit the model
model %>% fit(
  steps_per_epoch = 30,
  epochs = 5
)
```

Note that we don't pass `x_train` or `y_train` to `fit()`, rather the feature data (`batch$x`) is provided as the `tensor` argument to `layer_input()` and the response data (`batch$y`) is provided as the `target_tensors` argument to `compile()`. The training data is directly wired into the TensorFlow graph built for the model.

Note also that rather than calling the Keras `to_categorical()` function to one-hot encode the "Species" field, we do this instead in a `dataset_map()` operation that calls the `tf$one_hot()` function. 

## Batch Iteration

In the [Reading Batches] section above we demonstrated evaluating the tensors returned from `iterator_get_next()`. If you want to iterate over all batches of a dataset, you will need to detect the end of the iteration. This is handled automatically when you use tfestimators or keras, however if you are dealing with the tensors directly you need to do this explicitly.

### Infinite Batches

One approach to iteration is to create a dataset that yields batches infinitely (traversing the dataset multiple times with different batches randomly drawn). In this case you'd use another mechanism like a global step counter or check for a learning plateau. For example:

```{r}
library(tfdatasets)
dataset <- csv_dataset("training.csv") %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl) %>% 
  dataset_shuffle(5000) %>% 
  dataset_batch(128) %>% 
  dataset_repeat() # repeat infinitely

batch <- iterator_get_next(dataset)

steps <- 200
for (i in 1:steps) {
  # use batch$x and batch$y tensors
}
```

The call to `dataset_repeat()` with no `count` parameter requests that the dataset be traversed infinitely.

### Detecting Completion

Another approach to iteration is to detect when all batches have been yielded from the dataset. When the batch tensor reaches the end of iteration a runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in the `with_dataset_iterator()` function. For example:

```{r}
library(tfdatasets)
dataset <- csv_dataset("mtcars.csv") %>% 
  dataset_prepare(x = c(mpg, disp), y = cyl) %>% 
  dataset_batch(128) %>% 
  dataset_repeat(10)
  
batch <- iterator_get_next(dataset)
with_dataset_iterator({
  while(TRUE) {
    # use batch$x and batch$y tensors
  }
})
```



